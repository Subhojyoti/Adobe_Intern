%\subsection{Noise-Free Setting}


%\begin{assumption}
%\label{assm:d-indep}
%We assume that any $d$ sets of rows or columns are independent in $\bar{R}$.
%\end{assumption}

\algblock{ArmElim}{EndArmElim}
\algnewcommand\algorithmicArmElim{\textbf{\em Arm Elimination}}
 \algnewcommand\algorithmicendArmElim{}
\algrenewtext{ArmElim}[1]{\algorithmicArmElim\ #1}
\algrenewtext{EndArmElim}{\algorithmicendArmElim}

\algblock{ResParam}{EndResParam}
\algnewcommand\algorithmicResParam{\textbf{\em Reset Parameters}}
 \algnewcommand\algorithmicendResParam{}
\algrenewtext{ResParam}[1]{\algorithmicResParam\ #1}
\algrenewtext{EndResParam}{\algorithmicendResParam}

\algblock{ColRec}{EndColRec}
\algnewcommand\algorithmicColRec{\textbf{\em Column Reconstruction}}
 \algnewcommand\algorithmicendColRec{}
\algrenewtext{ColRec}[1]{\algorithmicColRec\ #1}
\algrenewtext{EndColRec}{\algorithmicendColRec}

\algblock{ColElim}{EndColElim}
\algnewcommand\algorithmicColElim{\textbf{\em Structured Column Elimination}}
 \algnewcommand\algorithmicendColElim{}
\algrenewtext{ColElim}[1]{\algorithmicColElim\ #1}
\algrenewtext{EndColElim}{\algorithmicendColElim}

\algblock{ColElimU}{EndColElimU}
\algnewcommand\algorithmicColElimU{\textbf{\em User Column Elimination}}
 \algnewcommand\algorithmicendColElimU{}
\algrenewtext{ColElimU}[1]{\algorithmicColElimU\ #1}
\algrenewtext{EndColElimU}{\algorithmicendColElimU}

\algblock{Explore}{EndExplore}
\algnewcommand\algorithmicExplore{\textbf{\em Explore}}
 \algnewcommand\algorithmicendExplore{}
\algrenewtext{Explore}[1]{\algorithmicExplore\ #1}
\algrenewtext{EndExplore}{\algorithmicendExplore}


\algblock{Exploit}{EndExploit}
\algnewcommand\algorithmicExploit{\textbf{\em Exploit}}
 \algnewcommand\algorithmicendExploit{}
\algrenewtext{Exploit}[1]{\algorithmicExploit\ #1}
\algrenewtext{EndExploit}{\algorithmicendExploit}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

%\newcommand\Algphase[1]{%
%\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
%\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
%\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
%}

%We propose two algorithms, one each for noise free and noisy setting. These algorithms are 

We propose the GLBUCB algorithm based on UCB-Improved, which is an arm elimination algorithm from \citet{auer2010ucb} and is suitable for the stochastic bandit setting. Both these algorithms are phase based column (arm) elimination algorithms where in each phase we select the surviving columns some number of times  and then eliminate some sub-optimal columns based on an elimination criteria. 

\textbf{Initialization:} The algorithm is initialized with the estimate $\hat{R}_{i,j}=0, \forall i\in[K], j\in[L]$. In the $m$-th phase, it denotes  the set of surviving columns as $\B_m$. The rows (users) are divided into equivalence classes which are contained in $\C$.  $N_m$ denotes the phase length for the $m$-th phase and each phase length consist of $\gamma |B_m| \ell_m $ timesteps, where $ \gamma = f(d) \geq d^2 $ is the exploration parameter which is a function of the rank $d$ and $\ell_m $ is the number of pulls GLBUCB allocates for each of the $\gamma$ columns. 

\textbf{Equivalence class and initialization:} Each equivalence class is denoted as $\G_b = \left\lbrace i\in \left[1+(b-1)d, bd\right]\right\rbrace$, where $b$ is indexed from $1,2,\ldots , \frac{K}{d}$ and these classes are contained in $\C$. At the start of the algorithm $\gamma$ random columns are chosen for all users in each $\G_b,  \forall b\in \left[ 1, \frac{K}{d}\right]$ and these are contained in $\Z_{\G_b,m}$.

\textbf{Optimistic greedy sampling:} Unlike UCB-Improved, GLBUCB does not pull all surviving columns in the equivalence class for each given user an equal number of times as this wastes a large number of pulls in exploration. Rather, for the entire phase length $N_m$, it behaves greedily (like UCB1) and selects the column $j_0 \in \argmax_{j\in_{\B_m\cap \Z_{\G_b,m}}}\big\lbrace \hat{R}(i_t,j)  + U_m(\epsilon_m, n_{i_t,j})\big\rbrace $, where $i_t$ is the user revealed by nature and $i_t\in \G_b$. The confidence interval $ U_m(\epsilon_m, n_{i_t,j})$ makes sure that sufficient exploration is conducted amongst the columns in $\Z_{\G_b,m}$. 

\textbf{Structured Column Elimination:} In the column elimination sub-module GLBUCB eliminates a sub-optimal column by making sure that it is not one of the $d$-best columns. Moreover, in the column elimination sub-module, the confidence interval $U_m(\epsilon_m, n_m)$  helps in eliminating a sub-optimal column with high probability in the noisy setting. 

\textbf{Reset Parameters:} The reset parameters sub-module, can be divided into two parts: 
\begin{itemize}
\item\textbf{Information share:} GLBUCB reconstructs the equivalence classes such that the each $\G_b,\forall b\in\left[ 1, \frac{K}{d}\right]$ contains the $d$ best performing  columns amongst all the users $i\in[K]$. This is achieved by first selecting the  column $j\in \argmax_{j\in\Z_{\G_b,m}}\lbrace \hat{R}(i,j)  + U_m(\epsilon_m, n_{i,j})\rbrace, \forall i\in[K], i\in\G_b$ and then selecting the $d$ most frequent such columns. These $d$-best columns are included in $\Z_{\G_b,m+1}, \forall b\in\left[1,\frac{K}{d}\right]$ and then for each $\G_b$ the remaining $\gamma - d$ columns are selected uniform randomly from $\B_m \cap \Z_{\G_b,m}$.

\item\textbf{Increase exploration: } GLBUCB increases the exploration bonus for the next phase so that more exploration is conducted for the surviving columns. 

\end{itemize}

Finally, if the algorithm has eliminated $L-d$ columns, for the user $i_t$ revealed by the nature, it always selects the column $j^*_t$, where $j_t^* \leftarrow \argmax_{j\in[\B_m]} {\hat{R}(i_t,j)}$. The pseudo-code of this is shown in Algorithm \ref{alg:NGLB}. 

% it fully explores the $d$ best columns (in the noise free setting) and for all the users

%Whereas, in the noisy setting, the GLB-UCB runs the UCB1 algorithm for the remaining $d$ columns.

%Note, that for these two sub-modules, the noise free and noisy setting has two different approaches which will be explained in subsection \ref{alg:noisefree} and \ref{alg:noisy} respectively.

%\subsection{Noise Free Setting}
%\label{alg:noisefree}
%
%In the noise free setting, since at every pull of a column $j$ for a user $i$, the algorithm observes the expected reward $\hat{R}_{i,j} = \bar{R}_{i,j}$, so $n_0 =  n_m = 1, \forall m$. Furthermore, we do not require any confidence interval in the noise free setting for column elimination sub-module. The pseudo-code of this is shown in Algorithm \ref{alg:NFGLB}.

\begin{algorithm}[!th]
\caption{GLBUCB}
\label{alg:NGLB}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $T$, $Rank(\bar{R}) = d$.
\State {\bf Explore Parameters:} $\gamma \geq d^2, \alpha \geq  \dfrac{1}{2}, \psi \geq 1$.
\State {\bf Definition:} $U_m(\epsilon_m, n_{i,j}) = \sqrt{\dfrac{\alpha\log(\psi T\epsilon_m^2)}{2n_{i,j}} }$
\State {\bf Initialization:} $\hat{R}(i,j) \leftarrow 0, \forall i\in [K], \forall j\in [L]$, $m=0$, $\B_0 \leftarrow \A$, $\epsilon_0=1$, $\ell_0 =  \dfrac{2\log(\psi T\epsilon_{0}^2)}{\epsilon_{0}} $ and $N_0 =  K\gamma \ell_0 $.
\ForEach{$b\in \left[1,\frac{K}{d}\right]$} \Comment{Create equivalence class $\C$}
\State  $\G_{b}\leftarrow \left\lbrace i\in \left[1+(b-1)d, bd\right]\right\rbrace$ and $\C\leftarrow \C\cup \G_{b}$.
\EndFor
\State $Z_{\G_b, 0}\leftarrow$ Select $\gamma$ random columns for each equivalence class $\G_b\in\C, \forall b\in \left[1,\frac{K}{d}\right] $. %and put then in $Z_{\G_b, 0}$.
\For{$t=1,..,T$}	
\State Nature reveals $i_t$ s.t. $i_t \leftarrow (t \mod K) + 1$ \Comment{Round-Robin}
\If{$|\B_m| > d$} 
\If{$t \leq N_m$}  \Comment{Till end of Phase do UCB1 on $ \Z_{\G_b,m}$}
\State Choose $j_0 \in \argmax_{j\in_{\Z_{\G_b,m}}}\left\lbrace \hat{R}(i_t,j)  + U_m(\epsilon_m, n_{i_t,j})\right\rbrace $, where $i_t\in \G_b$.
%, observe $r_t(i_t,j_0)\sim \mathcal{N}(\bar{R}(i_t,j_0),\sigma^2)$ and $\hat{R}(i_t,j_0)\leftarrow \dfrac{\sum_{s=1}^{t}r_s(i_t,j_0)[\mathbb{I}_{s,i_t} = j_0]}{n_{i_t,j_0}}$.
\Else \Comment{End of phase, do elimination and reset parameters}
\ColElim
\State \ForEach{$\G_b\in\C$}
\State \While{$\exists j\in\B_m \textbf{ s.t. } {\forall i\in \G_b: \hat{R}(i,j) + U_m(\epsilon_m, n_{i,j}) < \max_{j'\in\B_{m}\setminus j}\left\lbrace\hat{R}(i,j') - U_m(\epsilon_m, n_{i,j'})\right\rbrace}$}
\State $\B_m \leftarrow \B_m \setminus \lbrace j \rbrace$. 
%\EndIf
\EndWhile
\EndFor
\EndColElim
\ResParam
\ForEach{$i \in [K]$} \Comment{Find best $d$ arms}
\State $\D_{m}(i) \leftarrow \argmax_d\left\lbrace \hat{R}(i,j)  + U_m(\epsilon_m, n_{i,j})\right\rbrace, \forall j\in \B_m$
\EndFor
\ForEach{$\G_b\in\C$} 
\State $\Z_{\G_b,m+1}\leftarrow$ $d$ most frequent $j\in\D_m$ and select $\gamma - d$ different columns uniform randomly from $\B_m\cap\Z_{\G_b,m}$.
\EndFor
\State $\epsilon_{m+1} \leftarrow \dfrac{\epsilon_m}{2}$ and $\ell_{m+1} \leftarrow \dfrac{2\log(\psi T\epsilon_{m+1}^2)}{\epsilon_{m+1}}$
\State $N_{m+1}\leftarrow t + K \gamma \ell_{m+1} $ and $m \leftarrow m + 1$.
\EndResParam
\EndIf
\Else \Comment{Till $T$ do UCB1 on remaining $d$ arms}
\State  Select column $j_t^*\in \argmax_{j\in\B_m} \left\lbrace \hat{R}(i_t,j) + U_m(\epsilon_m, n_{i_t,j})\right\rbrace$ 
%  observe $r_t(i_t,j_t^*)\sim \mathcal{N}(\bar{R}(i_t,j_t^*),\sigma^2)$ where $j_t^* \leftarrow
%and $\hat{R}(i_t,,j_t^*)\leftarrow \dfrac{\sum_{s=1}^{t}r_s(i_t,j_t^*)[\mathbb{I}_{s,i_t} = j_t^*]}{n_{i_tj_t^*}}$.
\EndIf
\EndFor
\end{algorithmic}
%\vspace*{-0.42em}
\end{algorithm}


%\subsection{Noisy setting}
%\label{alg:noisy}

%In the noisy setting, at every pull the algorithm observes a random reward which is drawn from the distribution $\N(\bar{R}_{i,j}, \sigma^2)$, so the algorithm samples each column more number of times ($n_m$) in each phase and increases the exploration from phase to phase. Moreover, in the column elimination sub-module, the confidence interval $U_m(\epsilon_m, n_m)$  helps in eliminating a sub-optimal column with high probability in the noisy setting. The pseudo-code of this is shown in Algorithm \ref{alg:NGLB}.


%\begin{algorithm}[!th]
%\caption{Noisy GLB-UCB}
%\label{alg:NGLB}
%\begin{algorithmic}[1]
%\State {\bf Input:} Time horizon $T$, $Rank(\bar{R}) = d$.
%\State {\bf Explore Parameter:} $\gamma = \lceil\sqrt{K} \rceil$, $\psi = T$.
%\State {\bf Definition:} $U_m(\epsilon_m, n_m) = \sqrt{\dfrac{\psi\log(T\epsilon_m^2)}{2n_m} }$
%\State {\bf Initialization:} $\forall i\in [K], j\in [L], \hat{R}(i,j) \leftarrow 0$, $m=0$, $\B_0 \leftarrow \A$, $\Z_0 \leftarrow \emptyset$, $\C \leftarrow \emptyset$,  $i_0=1$, $j_0=1$ , $\epsilon_0 = 1$, $M=\dfrac{1}{2}\log_2\left( \frac{T}{e}\right)$, $n_0 = \dfrac{2\log(\psi T\epsilon_{0}^2)}{\epsilon_{0}} $, $N'_0 = K n_0 d$ and $N_0 = N'_0 + \gamma |\B_0|n_0$.
%%\State Create set $\C$ of equivalence classes such that 
%\ForEach{$b\in \left[0,\frac{K}{\gamma}-1\right]$} (Create equivalence class $\C$)
%\State  $\G_{b+1}\leftarrow \left\lbrace i\in \left[\frac{bK}{\gamma}+1,\frac{(b+1)K}{\gamma}\right]\right\rbrace$ and $\C\leftarrow \C\cup \G_{b+1}$.
%\EndFor
%\For{$t=1,..,T$}	
%\State Nature reveals $i_t$ such that $i_t \leftarrow (t \mod K) + 1$ (Round-Robin).
%\If{$|\B_m| > d$} 
%\If{$t \leq N_m$ and $t \leq N'_m$} \textbf{ (Phase Exploit) }
%\State Choose $j_0\leftarrow \argmax_{j\in_{B_m}}\left\lbrace \hat{R}(i_t,j)  + \sqrt{\dfrac{2\log T}{n_{i_t,j}}}\right\rbrace $, observe $r_t(i_t,j_0)\sim \mathcal{N}(\bar{R}(i_t,j_0),\sigma^2)$ and $\hat{R}(i_t,j_0)\leftarrow \dfrac{\sum_{s=1}^{t}r_s(i_t,j_0)[\mathbb{I}_{s,i_t} = j_0]}{n_{i_t,j_0}}$.
%\ElsIf{$t \leq N_m$ and $t > Nx_m$} \textbf{ (Phase Explore) }
%\If{$i_0 \leq \gamma $}
%\State Choose $j_0$, observe $r_t(i_0,j_0)\sim\mathcal{N}(\bar{R}(i_0,j_0),\sigma^2) $ and $\hat{R}(i_0,j_0)\leftarrow \dfrac{\sum_{s=1}^{t}r_s(i_0,j_0)[\mathbb{I}_{s,i_0} = j_0]}{n_{i_0,j_0}}$.
%\State $i_0 \leftarrow i_0 + 1$.
%\Else
%\State $\Z_m \leftarrow \Z_m \cup \lbrace j_0\rbrace$
%\State Choose $j_0\in\B_m\setminus \Z_m$ uniform randomly and $i_0 \leftarrow 1$.
%\EndIf
%\Else
%\ColElim
%\State \ForEach{$\G_b\in\C$}
%\State \While{$\exists j\in\B_m \textbf{ such that } {\forall i\in \G_b: \hat{R}(i,j) + U_m(\epsilon_m, n_m) < \max_{j'\in\B_{m}\setminus j}\lbrace\hat{R}(i,j') - U_m(\epsilon_m, n_m)\rbrace}$}
%\State $\B_m \leftarrow \B_m \setminus \lbrace j \rbrace$. 
%%\EndIf
%\EndWhile
%\EndFor
%\EndColElim
%\ResParam
%\State $\epsilon_{m+1} \leftarrow \dfrac{\epsilon_m}{2}$ and $n_{m+1} \leftarrow \dfrac{2\log(\psi T\epsilon_{m+1}^2)}{\epsilon_{m+1}}$
%\State $N'_{m+1}\leftarrow t + K n_{m+1} d$
%\State $N_{m+1} \leftarrow N'_{m+1} + \gamma |\B_m|n_{m+1}$ and $m \leftarrow m + 1$.
%\EndResParam
%\EndIf
%\Else \textbf{ (Full Exploit) }
%%\Select Explore all $j\inB_{m}$ fully.
%\State  Select column $j_t^*$, observe $r_t(i_t,j_t^*)\sim \mathcal{N}(\bar{R}(i_t,j_t^*),\sigma^2)$ where $j_t^* \leftarrow \argmax_{j\in[\B_m]} \left\lbrace \hat{R}(i_t,j) + \sqrt{\dfrac{2\log T}{n_{i_t,j}}}\right\rbrace$ and $\hat{R}(i_t,,j_t^*)\leftarrow \dfrac{\sum_{s=1}^{t}r_s(i_t,j_t^*)[\mathbb{I}_{s,i_t} = j_t^*]}{n_{i_tj_t^*}}$.
%\EndIf
%\EndFor
%\end{algorithmic}
%%\vspace*{-0.42em}
%\end{algorithm}


