%Extending the discussion in the previous section, we propose a Generalized Latent Bandit model where each $v_{a,b}$ can be considered as a mixture of several $\lbrace v_{a,c}\rbrace_{c\in\C}$. This is a harder problem than Latent Bandit model because now the rows in each cluster are not exactly identical but the index of the optimal arm (column) is similar in each row.
	
	We define $[n] = \lbrace 1,2,\ldots, n\rbrace$ and for any two sets $A$ and $B$, $A^B$ denotes the set of all vectors who take values from $A$ and are indexed by $B$. Let, $R\in [0,1]^{K\times L}$ denote any matrix, then $R(I,:)$ denote any submatrix of $k$ rows such that $I\in[K]^k$ and similarly $R(:,J)$ denote any submatrix of $j$ columns such that $J\in[L]^{j}$.
	
	Let $\bar{R}$ be reward matrix of  dimension $K\times L$ where $K$ is the number of user or rows and $L$ is the number of arms or columns. Also, let us assume that this matrix  $\bar{R}$ has a low rank structure of rank $d << \min\lbrace L,K\rbrace$. Let $U$ and $V$ denote the latent matrices for the users and items, which are not visible to the learner such that,
\begin{align*}
	\bar{R} = UV^{\intercal} \textbf{ \hspace*{4mm}   s.t.   \hspace*{4mm}} U\in [ \mathbb{R}^+ ]^{K\times d} \textbf{, } V\in  [0,1]^{L\times d} 
\end{align*}	  
	
	Furthermore, we put a constraint on $V$ such that, $\forall j\in [L]$, $ \norm{V(j,:)}_1 \leq 1$. 
	
	
\begin{assumption}
\label{assm:1}
We assume that there exists $d$-column base factors, denoted by $V(J^*,:)$, such that all rows of $V$ can be written as a convex combination of $V(J^*,:)$ and the zero vector and $J^* = [d]$. We denote the column factors by $V^* = V(J*,:)$. Therefore, for any $i\in [L]$, it can be represented by
\begin{align*}
V(i,:) = a_i V(J^*,:) , 
\end{align*}
where $\exists a_i\in [0,1]^{d}$ and $ \norm{a_i}_1 \leq 1$.
\end{assumption}

%In this paper, in addition to the noisy setting explained in section \ref{intro} we first analyze the proposed algorithm in the easier noise free setting. In the noise free setting, the nature reveals the row $i_t$, and when the learner selects the column $j_t$, it observes the mean of the distribution $\bar{R}(i_t,j_t)$.

%\begin{assumption}
%\label{assm:round-robin}
%We assume that nature is revealing the user $i$ in $\bar{R}(i,:), \forall i\in [K]$  in a Round-Robin fashion such that at timestep $t$, nature reveals $i_t = (t \mod K) + 1$.
%\end{assumption}

\begin{assumption}
\label{assm:d-items}
For each user $i_t$ revealed by the nature at round $t$, the learner is allowed to select atmost $d$-items, where $d$ is the rank of the matrix $\bar{R}$.
\end{assumption}

The above assumption \ref{assm:d-items} can be conceptualized in this real-world scenario where the learner has  to suggest movies to users and each movie belongs to a different genre (say thriller, romance, comedy, etc). So, the learner can suggest $d$ movies belonging to different genres to each user, and the user can click one, or both, or none of the recommended movies.


The main goal of the learning agent is to minimize the cumulative regret until the end of horizon $n$. We define the cumulative regret, denoted by $\mathcal{R}_n$ as,

\begin{align*}
\mathcal{R}_n = \sum_{t=1}^{n}\bigg\lbrace \sum_{z=1}^{d} \bigg( r_{z,t}\left(i_{t}, j^*_{z,t} \right) - r_{z, t}\left( i_{t}, j_{z,t}\right)\bigg)\bigg\rbrace
\end{align*}

where, $j^*_{z,t} = \argmax_{j\in [L]}\lbrace \bar{R}(i_t,j)\rbrace$ and $j_{z,t}$ be the suggestion of the learner for the $i_t$ -th user for  $z=1,2,\ldots, d$. Note that $r_{t}\left(i_t, j^*_{z,t} \right)\sim \mathcal{N}(\bar{R}\left(i_t, j^*_{z,t} \right),\sigma^2)$ and $r_{t}\left(i_t, j_{z,t} \right)\sim \mathcal{N}(\bar{R}\left(i_t, j_{z,t} \right), \sigma^2)$. Taking expectation over both sides, we can show that,

\begin{align*}
\E[\mathcal{R}_n] = \E\left [ \sum_{t=1}^{n}\bigg\lbrace\sum_{z=1}^{d} \bigg( r_{z,t}\left(i_t, j^*_{z,t} \right) - r_{z, t}\left( i_t, j_{z, t}\right)\bigg)\bigg\rbrace\right] = \E\left [ \sum_{t=1}^{T} \sum_{z=1}^{d} \bigg( N_{i_t,j_{z,t}}\bigg) \right ]\Delta_{i_t,j_{z,t}}
\end{align*}

where, $\Delta_{i_t,j_{z,t}} = \bar{R}(i_t,j^*_{z,t}) - \bar{R}(i_t,j_{z,t})$ and $N_{i_t,j_{z,t}}$ is the number of times the learner has observed the $j_{z,t}$-th item for the $i_t$-th user for $z=1,2,\ldots, d$. Let, $\Delta = \min_{i\in[K],j\in[L]}\lbrace \Delta_{i,j}\rbrace$ be the minimum gap over all the user, item pair in $\bar{R}$.
