
\algblock{SampleRule}{EndSampleRule}
\algnewcommand\algorithmicSampleRule{\textbf{\em Sampling Rule}}
 \algnewcommand\algorithmicendSampleRule{}
\algrenewtext{SampleRule}[1]{\algorithmicSampleRule\ #1}
\algrenewtext{EndSampleRule}{\algorithmicendSampleRule}

\algblock{UpdateRule}{EndUpdateRule}
\algnewcommand\algorithmicUpdateRule{\textbf{\em Update Rule}}
 \algnewcommand\algorithmicendUpdateRule{}
\algrenewtext{UpdateRule}[1]{\algorithmicUpdateRule\ #1}
\algrenewtext{EndUpdateRule}{\algorithmicendUpdateRule}


\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}


\algnewcommand\algorithmicWith{\textbf{With}}
\algdef{S}[FOR]{With}[1]{\algorithmicWith\ #1\ \algorithmicdo}


\algnewcommand\algorithmicOrWith{\textbf{Or With}}
\algdef{S}[FOR]{OrWith}[1]{\algorithmicOrWith\ #1\ \algorithmicdo}

%Without loss of generality, in the noiseless setting we can consider the first $[d] \subseteq [L]$ as the best set of columns. Then the $j$-th best column in $[d]$ is given by the estimation:-
%\begin{align}
%\hat{R}_j = \dfrac{1}{K}\max_{j\in[d]}\lbrace \bar{R}(:, j)\rbrace -  \dfrac{1}{K}\max_{j\in[d]}\lbrace\bar{R}(:, j-1) \rbrace
%\end{align}
%
%and $\bar{R}(i, 0) = 0, \forall i\in [K]$.


Let $\bar{R} = UV^{\intercal}$, where $U$ is non-negative and $V$ is hott topics. Let $j^\ast_1$ and $j^\ast_2$ be the indices of hott-topics vectors. Then
 \begin{align*}
 (j^\ast_1, j^\ast_2) = \arg\max_{j_1, j_2 \in [L]} f(\{j_1, j_2\}),
 \end{align*}
  
 
where $f(S) = \dfrac{1}{K}\sum_{i \in [K]} \max_{j \in S} R(i, j)$
 
The key observation is that $f$ is monotone and submodular in S. Therefore, the problem of learning $j_1, j_2$ online is an online submodular maximization problem.

So, when $d=2$, $|\B_t| = 2$ and there are two EXP3 Column-Bandits. 

After observing the reward $r_1, r_2$ for $j_1, j_2 \in \B_t$ we update,

$EXP_1$, $\hat{r}_{1,j_1} = r_1 $.

$EXP_2$, $\hat{r}_{2,j_2} = \max\lbrace r_1, r_2\rbrace  -  r_1$.
 
 

%$\epsilon$-greedy policy

%We propose the meta-bandit algorithm algorithm based on UCB-Improved, which is an arm elimination algorithm from \citet{auer2010ucb} and is suitable for the stochastic bandit setting. Both these algorithms are phase based column (arm) elimination algorithms where in each phase we select the surviving columns some number of times  and then eliminate some sub-optimal columns based on an elimination criteria. 
%
%\textbf{Initialization:} The algorithm is initialized with the estimate $\hat{R}_{i,j}=0, \forall i\in[K], j\in[L]$. In the $m$-th phase, it denotes  the set of surviving columns as $\B_m$. The rows (users) are divided into equivalence classes which are contained in $\C$.  $N_m$ denotes the phase length for the $m$-th phase and each phase length consist of $\gamma |B_m| \ell_m $ timesteps, where $ \gamma = f(d) \geq d^2 $ is the exploration parameter which is a function of the rank $d$ and $\ell_m $ is the number of pulls GLBUCB allocates for each of the $\gamma$ columns. 
%
%\textbf{Equivalence class and initialization:} Each equivalence class is denoted as $\G_b = \left\lbrace i\in \left[1+(b-1)d, bd\right]\right\rbrace$, where $b$ is indexed from $1,2,\ldots , \frac{K}{d}$ and these classes are contained in $\C$. At the start of the algorithm $\gamma$ random columns are chosen for all users in each $\G_b,  \forall b\in \left[ 1, \frac{K}{d}\right]$ and these are contained in $\Z_{\G_b,m}$.
%
%\textbf{Optimistic greedy sampling:} Unlike UCB-Improved, GLBUCB does not pull all surviving columns in the equivalence class for each given user an equal number of times as this wastes a large number of pulls in exploration. Rather, for the entire phase length $N_m$, it behaves greedily (like UCB1) and selects the column $j_0 \in \argmax_{j\in_{\B_m\cap \Z_{\G_b,m}}}\big\lbrace \hat{R}(i_t,j)  + U_m(\epsilon_m, n_{i_t,j})\big\rbrace $, where $i_t$ is the user revealed by nature and $i_t\in \G_b$. The confidence interval $ U_m(\epsilon_m, n_{i_t,j})$ makes sure that sufficient exploration is conducted amongst the columns in $\Z_{\G_b,m}$. 
%
%\textbf{Structured Column Elimination:} In the column elimination sub-module GLBUCB eliminates a sub-optimal column by making sure that it is not one of the $d$-best columns. Moreover, in the column elimination sub-module, the confidence interval $U_m(\epsilon_m, n_m)$  helps in eliminating a sub-optimal column with high probability in the noisy setting. 
%
%\textbf{Reset Parameters:} The reset parameters sub-module, can be divided into two parts: 
%\begin{itemize}
%\item\textbf{Information share:} GLBUCB reconstructs the equivalence classes such that the each $\G_b,\forall b\in\left[ 1, \frac{K}{d}\right]$ contains the $d$ best performing  columns amongst all the users $i\in[K]$. This is achieved by first selecting the  column $j_0\in \argmax_{j\in\Z_{\G_b,m}}\big\lbrace \hat{R}(i,j)  + U_m(\epsilon_m, n_{i,j})\big\rbrace, \forall i\in[K], i\in\G_b$ and then selecting the $d$ most frequent such columns. These $d$-best columns are included in $\Z_{\G_b,m+1}, \forall b\in\left[1,\frac{K}{d}\right]$ and then for each $\G_b$ the remaining $\gamma - d$ columns are selected uniform randomly from $\B_m \cap \Z_{\G_b,m}$.
%
%\item\textbf{Increase exploration: } GLBUCB increases the exploration bonus for the next phase so that more exploration is conducted for the surviving columns. 
%
%\end{itemize}
%
%Finally, if the algorithm has eliminated $L-d$ columns, for the user $i_t$ revealed by the nature, it always selects the column $j^*_t$, where $j_t^* \leftarrow \argmax_{j\in[\B_m]} {\hat{R}(i_t,j)}$. The pseudo-code of this is shown in Algorithm \ref{alg:NGLB}. 



\begin{algorithm}[!th]
\caption{Low Rank Bandit Strategy}
\label{alg:mLRBS}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $n$, $Rank(\bar{R}) = d$.
\For{$t=1,..,n$}	
\State Nature reveals user $i_t$.  \Comment{Nature chooses user}
\State Column-Bandits suggests $\B_t \subseteq [L]$ items. $|\B_t| = d$
%\For{$z=1,..,d$}	
\If{Exploration condition satisfied}
\State User Bandits suggests each item in $\B_t$, once to user $i_t$ and receive feedback.
\State Update Column-Bandits and User Bandits on feedback  received.
%\EndFor
\Else
\State Suggest best item in $\B_t$ d times to user $i_t$ and receive feedback.
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!th]
\caption{Low Rank Bandit Greedy (LRG)}
\label{alg:mLRBS1}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $n$, $Rank(\bar{R}) = d$.
\State {\bf Explore Parameters:} $\epsilon\in (0,1)$.
\For{$t=1,..,n$}	
\State Nature reveals user $i_t$.  \Comment{Nature chooses user}
\State Column-EXP3 suggests $\B_t\subseteq [L]$ items. $|\B_t| = d$
\With{$\epsilon$ probability}	 \Comment{Exploration}
\State User Bandit suggests each arm $j\in\B_t$ once to user $i_t$ and receive feedback.
\EndFor
\OrWith{$(1-\epsilon)$ probability} \Comment{Exploitation}
\State User Bandit suggests arm $j\in \argmax_{j\in\B_t}\left\lbrace \hat{R}(i_t,j)\right\rbrace$, $d$ times to user $i_t$ and receive feedback.
\EndFor
\State Update Column-Bandits and User Bandit on feedback received.
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[!th]
\caption{Low Rank Bandit UCB (LRUCB)}
\label{alg:mLRBS2}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $n$, $Rank(\bar{R}) = d$.
\State {\bf Definition:} $U(i,j) = \sqrt{\dfrac{2\log n}{N_{i,j}}}$.
\For{$t=1,..,n$}	
\State Nature reveals user $i_t$.  \Comment{Nature chooses user}
\State Column-EXP3 suggests $\B_t\subseteq [L]$ items. $|\B_t| = d$
\If{$\left(\hat{R}(i_t,j) - U(i_t,j) \leq \hat{R}(i_t,j') + U(i_t,j')\right), \forall j,j' \in \B_t$}	 \Comment{Confidence interval overlap, Exploration}
\State User Bandit suggests each arm $j\in\B_t$ once to user $i_t$ and receive feedback.
\Else \Comment{Exploitation}
\State User Bandit suggests arm $j\in \argmax_{j\in\B_t}\left\lbrace \hat{R}(i_t,j) + U(i_t,j)\right\rbrace$, $d$ times to user $i_t$ and receive feedback.
\EndIf
\State Update Column-Bandits and User Bandits on feedback received.
\EndFor
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[!th]
\caption{Meta Low Rank Bandit Greedy(MLRG)}
\label{alg:mLRBG}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $T$, $Rank(\bar{R}) = d$.
\State {\bf Explore Parameters:} $\gamma \in (0,1]$.
\State {\bf Initialization:} Initialize all user experts $M_{i}\left( [L]\right), \forall i\in [K]$,  $\B_0\leftarrow \emptyset$, and MetaEXP$_{z}(\gamma, 0, \B_0),\forall z\in[d]$ .
\State Receive $\B_1$ from MetaEXP$_z(\gamma, 0, \B_0),\forall z\in[d]$ by sampling rule.
\For{$t=1,..,T$}	
\State Nature reveals $i_t$.  \Comment{Nature chooses user}
\With{$\epsilon$ probability}
\State Select all arms $j\in \B_t$ once, where $|\B_t| = d$.
\State Update MetaEXP$_{z}(\gamma, t),\forall z\in[d]$ by
\textit{update rule}. \Comment{Update MetaEXP$_{z}(\gamma, t),\forall z\in[d]$}
\State Receive new $\B_{t+1}$ from MetaEXP$_z(\gamma, t, \B_t),\forall z\in[d]$ by \textit{sampling rule}.
%\State Add $j_t$ to $\A_t$ and $r_{i_t,j_t}$ to $\X_t$.
%\If{ each arm in $\B_t$ pulled atleast once}
\EndFor
\OrWith{$1-\epsilon$ probability} \Comment{Run User Expert $M_{i_t}$ on $\B_t$ columns for $d$ times}
\State Select $j_t$ suggested by greedy $M_{i_t}\left( \B_t \right)$ for $d$ times, observe $r_{i_t,j_t}$ and update $M_{i_t}\left( \B_t \right)$. 
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}






\begin{algorithm}[!th]
\caption{Meta Low Rank Bandit UCB(MLRUCB)}
\label{alg:mLRB}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $T$, $Rank(\bar{R}) = d$.
\State {\bf Explore Parameters:} $\gamma \in (0,1]$.
\State {\bf Definition:} $U(i,j) = \dfrac{2\log T}{N_{i,j}}$.
\State {\bf Initialization:} Initialize all user experts $M_{i}\left( [L]\right), \forall i\in [K]$,  $\B_0\leftarrow \emptyset$, and MetaEXP$_{z}(\gamma, 0, \B_0),\forall z\in[d]$ .
\State Receive $\B_1$ from MetaEXP$_z(\gamma, 0, \B_0),\forall z\in[d]$ by sampling rule.
\For{$t=1,..,T$}	
\State Nature reveals $i_t$.  \Comment{Nature chooses user}
\If{$\left(\hat{R}(i_t,j) + U(i_t,j) \leq \hat{R}(i_t,j') - U(i_t,j')\right), \forall j,j' \in \B_t$}
\State Select all arms $j\in \B_t$ once, where $|\B_t| = d$.
\State Update MetaEXP$_{z}(\gamma, t),\forall z\in[d]$ by
\textit{update rule}. \Comment{Update MetaEXP$_{z}(\gamma, t),\forall z\in[d]$}
\State Receive new $\B_{t+1}$ from MetaEXP$_z(\gamma, t, \B_t),\forall z\in[d]$ by \textit{sampling rule}.
%\State Add $j_t$ to $\A_t$ and $r_{i_t,j_t}$ to $\X_t$.
%\If{ each arm in $\B_t$ pulled atleast once}
\Else \Comment{ Run User Expert $M_{i_t}$ on $\B_t$ columns for $d$ times}
\State Select $j_t$ suggested by $M_{i_t}\left( \B_t \right)$ for $d$ times, observe $r_{i_t,j_t}$ and update $M_{i_t}\left( \B_t \right)$. 
%\State $\A_{t+1}\leftarrow\emptyset$ and $\X_{t+1}\leftarrow\emptyset$.
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[!th]
\caption{MetaEXP$_{z}(\gamma, t, \B_t)$}
\label{alg:mEXP3}
\begin{algorithmic}[1]
\State {\bf Initialization:} Set $w_{z,j}(t=0) = 1, \forall j\in [L]$.
\State {\bf Sampling Rule:}
%\SampleRule
\ForEach{$j\in [L]$}
\State $p_{z,j}(t) = \left( 1 - \gamma \right)\dfrac{w_{z,j}(t)}{\sum_{j'=1}^{L}w_{z,j'}(t)} + \dfrac{\gamma}{L}$.
\EndFor
\State Suggest $j_{z,t} \notin \B_{t+1}$ by sampling according to the probabilities $p_{z,1}(t), p_{z,2}(t), \ldots, p_{z,L}(t)$.
%\State Set $w_{z',j}(t) \leftarrow 0$, in $MetaEXP_{z'}(\gamma, t),  \forall z' \in[d]$ and $z\neq z'$. Re-normalize the weights $w_{z',j},\forall j\in [L]$.
%\EndSampleRule
%\UpdateRule
\State {\bf Update Rule:} 
\ForEach{$j\in [L]$}
%\State Sample one reward $r_j(t)$ uniform randomly from $\X_t$ when $j = \A_t$
\State \begin{align*}
  \hat{r}_j(t)=\left\{
  \begin{array}{@{}ll@{}}
   \dfrac{r_{j}(t)}{p_{z,j}(t)}, & \text{if  } j = \ j_{z,t} \\
    0, & \text{otherwise}
  \end{array}\right.\\
 w_{z,j}(t+1) = w_{z,j}(t)\exp\left( \gamma \hat{r}_j(t)\right)
\end{align*} 
\EndFor
%\EndUpdateRule
\end{algorithmic}
\end{algorithm}

%\max\lbrace r_{i_{z,t},j_{z,t}}\rbrace - \max\lbrace r_{i_{z,t},j_{z-1,t}}\rbrace, & \text{if}\ j_{z_t} \in \A

