%Extending the discussion in the previous section, we propose a Generalized Latent Bandit model where each $v_{a,b}$ can be considered as a mixture of several $\lbrace v_{a,c}\rbrace_{c\in\C}$. This is a harder problem than Latent Bandit model because now the rows in each cluster are not exactly identical but the index of the optimal arm (column) is similar in each row.
	
	We define $[n] = \lbrace 1,2,\ldots, n\rbrace$ and for any two sets $A$ and $B$, $A^B$ denotes the set of all vectors who take values from $A$ and are indexed by $B$. Let, $R\in [0,1]^{K\times L}$ denote any matrix, then $R(I,:)$ denote any submatrix of $k$ rows such that $I\in[K]^k$ and similarly $R(:,J)$ denote any submatrix of $j$ columns such that $J\in[L]^{j}$.
	
	Let $\bar{R}$ be reward matrix of  dimension $K\times L$ where $K$ is the number of user or rows and $L$ is the number of arms or columns. Also, let us assume that this matrix  $\bar{R}$ has a low rank structure of rank $d << \min\lbrace L,K\rbrace$. Let $U$ and $V$ denote the latent matrices for the users and items, which are not visible to the learner such that,
\begin{align*}
	\bar{R} = UV^{\intercal} \textbf{ \hspace*{4mm}   s.t.   \hspace*{4mm}} U\in [ \mathbb{R}^+ ]^{K\times d} \textbf{, } V\in  [0,1]^{L\times d} 
\end{align*}	  
	
	Furthermore, we put a constraint on $V$ such that, $\forall j\in [L]$, $ ||V(j,:)||_1 \leq 1$. 
	
	
\begin{assumption}
\label{assm:1}
We assume that there exists $d$-column base factors, denoted by $V(J^*,:)$, such that all rows of $V$ can be written as a convex combination of $V(J^*,:)$ and the zero vector and $J^* = [d]$. We denote the column factors by $V^* = V(J*,:)$. Therefore, for any $i\in [L]$, it can be represented by
\begin{align*}
V(i,:) = a_i V(J^*,:) , 
\end{align*}
where $\exists a_i\in [0,1]^{d}$ and $ ||a_i||_1 \leq 1$.
\end{assumption}

In this paper, in addition to the noisy setting explained in section \ref{intro} we first analyze the proposed algorithm in the easier noise free setting. In the noise free setting, the nature reveals the row $i_t$, and when the learner selects the column $j_t$, it observes the mean of the distribution $\bar{R}(i_t,j_t)$.

\begin{assumption}
\label{assm:round-robin}
We assume that nature is revealing the user $i$ in $\bar{R}(i,:), \forall i\in [K]$  in a Round-Robin fashion such that at timestep $t$, nature reveals $i_t = (t \mod K) + 1$.
\end{assumption}

The main goal of the learning agent is to minimize the cumulative regret until the end of horizon $T$. We define the cumulative regret, denoted by $\mathcal{R}_T$ as,

\begin{align*}
\mathcal{R}_T = \sum_{t=1}^{T}\bigg\lbrace r_{t}\left(i_t, j^*_t \right) - r_{t}\left( i_t, j_{t}\right)\bigg\rbrace
\end{align*}

where, $j^*_t = \argmax_{j\in [L]}\lbrace \bar{R}(i_t,j)\rbrace$ and $j_t$ be the suggestion of the learner for the $i_t$ -th user. Note that $r_{t}\left(i_t, j^*_t \right)\sim \mathcal{N}(\bar{R}\left(i_t, j^*_t \right),\sigma^2)$ and $r_{t}\left(i_t, j_t \right)\sim \mathcal{N}(\bar{R}\left(i_t, j_t \right), \sigma^2)$. Taking expectation over both sides, we can show that,

\begin{align*}
\E[\mathcal{R}_T] = \E\left [ \sum_{t=1}^{T}\bigg\lbrace r_{t}\left(i_t, j^*_t \right) - r_{t}\left( i_t, j_{t}\right)\bigg\rbrace\right] = \E\left [ \sum_{t=1}^{T}  n_{i_t,j_t} \right ]\Delta_{i_t,j_t}
\end{align*}

where, $\Delta_{i_t,j_t} = \bar{R}(i_t,j^*_t) - \bar{R}(i_t,j_t)$ and $n_{i_t,j_t}$ is the number of times the learner has observed the $j_t$-th item for the $i_t$-th user. Let, $\Delta = \min_{i\in[K],j\in[L]}\lbrace \Delta_{i,j}\rbrace$ be the minimum gap over all the user, item pair in $\bar{R}$.
